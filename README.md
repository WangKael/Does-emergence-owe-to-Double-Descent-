# Does-emergence-owe-to-Double-Descent-
Does emergence owe to Double Descent?


Generally Speaking, the emergence is the patent of large language model, but it lacks theoretical proof.
According to some experiments that small model with more training data still have the emergence, it seems that the emergence results from SNR.
Connected to the Grokking phenomenon, we can guess the so-called emergence works in the **variance-reduction** pattern Of DoubleDescent

To prove this, just do this experiment: add several sub-model trained by splitted data together, and if the emergence still exist, we can say the VARIANCE dominates Emergence, i.e., the second stage of Double Descent is the reason for emergence

Lack of data and GPU, so sad..
